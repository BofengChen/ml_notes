{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定属性集，其中每个属性的重要程度可能不同。我们将属性称为**“特征”**，对**当前学习任务**有用的属性称为“相关特征”、没什么用的属性称为“无关特征”。从给定的特征集合中选择出相关特征子集的过程，称为**“特征选择”(feature selection)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择是一个重要的“数据预处理”过程，现实机器学习任务中，一般都是先进行特征选择，此后再训练学习器。进行特征选择的两个重要原因：\n",
    "\n",
    "1. 在现实任务中经常会遇到**属性过多导致的维数灾难问题**，若能从中选出重要特征，使得后续学习过程仅需在一部分特征上构建模型，维数灾难问题会大大减轻。从这个意义上讲，特征选择与第11章介绍的降维有相似的动机，对高维数据进行很好的处理；\n",
    "\n",
    "\n",
    "2. 去除不相关特征会降低学习任务的难度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将**特征子集搜索机制**与**子集评价机制**相结合，即可得到特征选择方法。如将前向搜索与信息熵结合，显然与决策树算法非常相似。下面我们将介绍这个方法。\n",
    "\n",
    "常见的特征选择方法大致分为三类：**过滤式(filter)、包裹式(wrapper)、嵌入式(embedding)。** 我们之后也会一一介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子集搜索与评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欲从初始特征集合中选取一个包含所有重要信息的特征子集，没有领域知识作为先验假设，只好遍历所有可能的子集，这在计算上是不可行的，会遭遇组合爆炸，特征个数稍多就无法进行。可行的办法是：\n",
    "\n",
    "    产生一个“候选子集”，评价好坏，基于评价结果产生下一个候选子集，再对其进行评价，持续下去直到无法找到更好的特征子集为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有两个关键环节：\n",
    "\n",
    "1. 如何根据评价结果获取下一个候选特征子集？\n",
    "2. 如何评价候选特征子集的好坏？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子集搜索\n",
    "\n",
    "子集搜索可以通过“前向”搜索、“后向”搜索、“双向”搜搜等。下面主要介绍下“前向”搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**“前向”搜索：**\n",
    "1. 给定特征集合 $\\{a_1,a_2, \\cdots,a_d\\}$，将每一个特征看做一个候选子集；\n",
    "2. 对 d 个候选单特征子集进行评价，假定 $\\{a_2\\}$ 最优（下面我们会介绍评价方法），于是将$\\{a_2\\}$ 作为第一轮的选定集；\n",
    "3. 在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集：\n",
    "\n",
    "    假定在 d-1 个候选中选出$\\{a_4\\}$ 使得 $\\{a_2, a_4\\}$ 最优，且优于 $\\{a_2\\}$，则将 $\\{a_2, a_4\\}$ 作为本轮的选定集；\n",
    "    \n",
    "    \n",
    "4. 持续进行步骤3直到第 k+1 轮中，最优的候选 (k+1) 特征子集不如上一轮的选定集，则停止生成候选子集；\n",
    "5. 将第 k 轮选定的 k 特征集合作为特征选择的结果。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和“前向”搜索通过“做加法”逐渐增加相关特征不同：\n",
    "\n",
    "1. **“后向”搜索**从完整的特征集合开始每次尝试去掉一个无关特征；\n",
    "2. **“双向”搜索**将前向和后向结合起来，每一轮逐渐增加选定特征（后续轮中确定不会被移除）、同时减少无关特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述的搜索策略显然都是贪心的，仅考虑本轮选定集最优，遗憾的是，如果不进行穷举搜索，这样的问题无法避免。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子集评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定数据集 D，假定 D 中第 i 类样本所占的比例为 $p_i,(i=1,2,\\cdots,|Y|)$。便于讨论，假设属性均为离散属性。\n",
    "\n",
    "对属性子集 A，假设里面有 k 个属性，根据其取值将 D 分成了 V 个子集$\\{D^1, D^2,\\cdots,D^V\\}$,每个子集中的样本在属性集 A 上取值相同，于是我们可计算属性子集 A 的信息增益\n",
    "\n",
    "$$ \\begin{split}Gain(A) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|} Ent(D^v)\n",
    "\\end{split}$$\n",
    "\n",
    "其中信息熵定义为\n",
    "\n",
    "$$ Ent(D)=-\\sum_{k=1}^{|Y|}p_k log_{2}p_k$$\n",
    "\n",
    "注：假设每个属性有 v 个可取值，则 $V=v^k$，这可能是一个很大的值，因此实践中通常是从子集搜索过程中前一轮属性子集的评价值出发来进行计算。相当于是每次只考虑第 k+1 轮属性增加相比第 k 轮属性子集对数据集 D 的信息增益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T08:16:26.052000Z",
     "start_time": "2019-02-01T08:16:26.044000Z"
    }
   },
   "source": [
    "## 过滤式选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤式方法先对数据集进行特征选择，然后再训练学习器，**特征选择过程与后续学习器无关，只是针对数据集本身的分布来决定不同特征重要程度的方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relief 方法（二分类问题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relief(Relevant Features) 是一种著名的针对二分类问题的过滤式特征选择方法，基本思想是：\n",
    "\n",
    "    该方法设计了一个“相关统计量”来度量每个特征的重要性。该统计量是一个向量，每个分量分别对应每个初始特征的重要性程度。特征子集的重要性对应着该统计量的相关分量和。可以通过指定阈值 M,然后选择比M 大的相关统计量分量所对应的特征即可；也可以指定欲选取的特征个数 k，然后按照统计量的大小进行排序，选择前 k 个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "阈值或者 k 的选择可以通过参数调优来估计。\n",
    "\n",
    "Relief 的关键是如何确定相关统计量。给定训练集 $D = \\{ (\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ... , (\\mathbf{x}_m, y_m)\\}$，对每个示例 $\\mathbf{x}_i$，先确定“猜中近邻”(near-hit)和“猜错近邻”(near-miss)：\n",
    "\n",
    "* **“猜中近邻”(near-hit)：**\n",
    "$$\\mathbf{x}_{i,nh} = \\min_{\\mathbf{x}_j} dist(\\mathbf{x}_{i}, \\mathbf{x}_{j}),\\quad y_i = y_j \\; and  \\; j\\neq i$$\n",
    "\n",
    "* **“猜错近邻”(near-miss)：**\n",
    "$$\\mathbf{x}_{i,nm} = \\min_{\\mathbf{x}_j} dist(\\mathbf{x}_{i}, \\mathbf{x}_{j}),\\quad y_i \\neq y_j \\; and  \\; j\\neq i$$\n",
    "\n",
    "注：$dist(\\cdot)$ 可自己决定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "near-hit 和 near-miss 就是针对示例 $\\mathbf{x}_i$ 的类别，分别从同类中和异类中各自寻找一个最近似的样例，这个和 SVM 中的“支持向量”的概念很像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在介绍统计量之前，我们要先介绍衡量两个样本$\\mathbf{x}_a, \\mathbf{x}_b$的属性/特征相似性的度量函数  $ diff (\\cdot,\\cdot)$:\n",
    "\n",
    "$$diff(\\mathbf{x}_a^j, \\mathbf{x}_b^j)=\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "diff_{dis}(\\mathbf{x}_a^j, \\mathbf{x}_b^j), \\quad& 如果 \\;j\\; 是离散属性\\\\\n",
    "diff_{con}(\\mathbf{x}_a^j, \\mathbf{x}_b^j), \\quad& 如果  \\;j\\; 是连续属性\n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "\n",
    "其中\n",
    "$$\\begin{split}\n",
    "&diff_{dis}(\\mathbf{x}_a^j, \\mathbf{x}_b^j)=\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "0 \\quad & if \\; \\mathbf{x}_a^j=\\mathbf{x}_b^j\\\\\n",
    "1 \\quad & else\n",
    "\\end{aligned}\n",
    "\\right.\\\\\n",
    "\\\\\n",
    "& diff_{con}(\\mathbf{x}_a^j, \\mathbf{x}_b^j) = |\\mathbf{x}_a^j-\\mathbf{x}_b^j|\n",
    "\\end{split}$$\n",
    "\n",
    "注：如果是连续属性， $\\mathbf{x}_a^j, \\mathbf{x}_b^j$ 已经归一化到 $[0,1]$ 之间。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据特征相似性的度量函数的定义，我们来定义关于属性 j 的重要性的统计量分量：\n",
    "\n",
    "$$\\delta^j = avg\\left(\\sum_{i} Rel_i^j\\right) \\tag{REL-0}$$\n",
    "\n",
    "其中 $Rel_i^j$ 为\n",
    "\n",
    "$$Rel_i^j = diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nm}^j)^2-diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nh}^j)^2\\tag{REL-1}$$\n",
    "\n",
    "则我们可以得到度量特征重要性的统计量为：\n",
    "\n",
    "$$ S = (\\delta^1, \\delta^2,\\cdots, \\delta^d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对样本 $\\mathbf{x}_i$和属性 j，从式 (REL-1) 看出：\n",
    "\n",
    "1. 如果 $Rel_i^j>0$，即 $diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nm}^j) > diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nh}^j)$，说明在属性 j 上，样本 $\\mathbf{x}_i$与其猜中近邻 $\\mathbf{x}_{i,nh}$ 距离更近，也就是说，属性 j 有助于区分同类和异类样本，所以通过保持 $Rel_i^j$ 为正，相当于增大属性 j 所对应的统计量分量；\n",
    "\n",
    "\n",
    "2. 如果 $Rel_i^j<0$，即 $diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nm}^j) < diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nh}^j)$，说明在属性 j 上，样本 $\\mathbf{x}_i$与其猜错近邻 $\\mathbf{x}_{i,nm}$ 距离更近，也就是说，属性 j 对区分同类和异类样本起负作用，所以通过保持 $Rel_i^j$ 为负，相当于减小属性 j 所对应的统计量分量。\n",
    "\n",
    "最后，针对不同样本得到的估计结果 $Rel_i^j$ 进行求和平均，就得到了特征重要性的统计量 S：分量值越大，则对应属性的分类能力就越强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，**对于 (REL-0) 只需要在数据集的采样上而不必在整个数据集上估计相关统计量。Relif 的时间开销随着 $采样次数\\times 原始特征数$ 线性增长，因此是一个运行效率很高的过滤式特征选择算法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relief-F 方法（多分类问题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设数据集 D 中的样本有 $|Y|$ 个类别。对于示例 $\\mathbf{x}_i$，若它属于第 k 类 ($k\\in\\{1,2,\\cdots,|Y|\\}$), 则在同类中求解 “猜中近邻”和 Relief 一样，稍有不同的是，作为延伸，我们针对第 $l$ 类 ($l\\in\\{1,2,\\cdots,|Y|;l \\neq k\\}$) 定义样本的“猜错近邻”：\n",
    "\n",
    "$$\\mathbf{x}_{i,l,nm} = \\min_{\\mathbf{x}_j} dist(\\mathbf{x}_{i}, \\mathbf{x}_{j}),\\quad y_i = k,\\; y_j = l \\; and \\; k\\neq l\\; and \\; j\\neq i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应 (REL-1) 式\n",
    "\n",
    "$$Rel_i^j = \\sum_{l\\neq k}\\left(p_{l}\\times diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,l,nm}^j)^2\\right)-diff(\\mathbf{x}_i^j, \\mathbf{x}_{i,nh}^j)^2\\tag{RELF-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包裹式选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 Relief 方法只考虑数据集本身不考虑后续学习器不同，包裹式特征选择直接**将最终的学习器性能作为特征子集的评价准则。**\n",
    "\n",
    "包裹式特征选择方法的特点：\n",
    "\n",
    "1. **从最终学习器性能上看**，包裹式特征选择直接针对给定学习器进行优化，比过滤式特征选择效果更好；\n",
    "2. **从计算开销上看**， 选择过程中需要多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LVW(Las Vegas Wrapper)是一个典型的包裹式特征选择方法，在**拉斯维加斯方法**框架下使用**随机策略**来进行子集搜索，以最终分类器的误差作为特征子集平价准则，算法描述如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/LVW.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：\n",
    "* 第8行是通过在数据集D上仅考虑特征子集 $A^\\prime$，使用交叉验证来估计学习器的误差;\n",
    "* 第9行是指，若它比当前特征子集 A 上的误差更小，或者误差相当但 $A^\\prime$ 中包含的特征更少，则将 $A^\\prime$ 保留下来。\n",
    "* 拉斯维加斯方法和蒙特卡洛方法都是随机化方法。两者的主要区别是：\n",
    "\n",
    "    1. 若有时间限制，拉斯维加斯方法或者给出满足要求的解，或者不给出解，蒙特卡洛方法一定给出解，虽然给出的解未必满足要求；\n",
    "    2. 若无时间限制，则两者都能给出满足要求的解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 LVW 算法中特征子集搜索采用了随机策略，每次特征子集评价都要训练学习器，计算开销很大，如果初始特征很多、停止条件控制参数 T 设置较大，则算法可能运行很长时间都达不到停止条件。也就是说，如果有运行时间限制，则有可能给不出解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入式选择与 L1 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面介绍的过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别；与此不同的是，**嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，在学习器训练过程中自动进行了特征选择**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定已标记数据集 $D = \\{ (\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ... , (\\mathbf{x}_m, y_m)\\},$ 其中 $ \\mathbf{x}\\in R^d,y\\in R$。我们考虑最简单的线性回归模型，以平方误差为损失函数，优化目标为\n",
    "\n",
    "$$\\min_{\\mathbf{w}}\\sum_{i=1}^m(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本特征很多，样本数相对较少时，上式容易陷入过拟合（数据量小，适合用简单的模型来拟合，或者说只需要几个关键特征来拟合，特征过多样本太少，样本稍微发生变化，学习器都会受到扰动）。为了缓解过拟合问题，可以对上式引入正则化项。若使用 $L_2$ 范数正则化，则有\n",
    "\n",
    "$$\\min_{\\mathbf{w}}\\sum_{i=1}^m(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2 + \\lambda ||\\mathbf{w}||_2^2 \\tag{EMB-1}$$.\n",
    "\n",
    "其中正则化参数 $\\lambda > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式 (EMB-1) 称为**“岭回归”(ridge regression)**，通过引入 $L_2$ 范数正则化，确实能够显著降低过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以将上式中的 $L_2$ 范数换成 $L_1$ 范数，则有\n",
    "\n",
    "$$\\min_{\\mathbf{w}}\\sum_{i=1}^m(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2 + \\lambda ||\\mathbf{w}||_1 \\tag{EMB-2}$$.\n",
    "\n",
    "其中正则化参数 $\\lambda > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式 (EMB-2) 称为**LASSO回归(least Absolute Shrinkage and Selection Operator)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 与 L2 的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 正则化与 L2 正则化都有助于降低过拟合风险，但相比于 L2 缩减各个参数的值，L1 更容易获得“稀疏”解，即它求得的 $\\mathbf{w}$ 会有更少的非零向量。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来直观地理解上面这句话。假设我们的样本 $\\mathbf{x}$ 只有 2 个属性，于是无论 (EMB-1) 或 (EMB-2) 求得的 $\\mathbf{w}$ 都只有两个分量，即 $w_1, w_2$。\n",
    "\n",
    "我们将两个属性作为坐标轴，然后：\n",
    "\n",
    "1. 在图中绘制出 (EMB-1) 和 (EMB-2) 的第一项的“等值线”，即在 $(w_1, w_2)$ 空间中平方误差取值相同的点的连线（想象一个有着平行纹路——纹路所在平面平行于 $(w_1, w_2)$ 所在平面——的锅被自上而下压扁在 $(w_1, w_2)$ 平面所形成的类似洋葱圈的图案）；\n",
    "\n",
    "\n",
    "2. 绘制 $L1$ 范数的等值线（道理类似于1）；\n",
    "\n",
    "\n",
    "3. 绘制 $L2$ 范数的等值线（道理类似于1）。\n",
    "\n",
    "如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/L1-L2.jpg' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(EMB-1) 和 (EMB-2) 的目的都是求解最小值，从图可知：\n",
    "\n",
    "\n",
    "* **越靠近左下方：两式中的第一项值越大，相应 L1/L2 的范数越小；**\n",
    "* **越靠近右上方：两式中的第一项值越小，相应 L1/L2 的范数越大；**\n",
    "\n",
    "所以 (EMB-1/2) 的解要在两者之间折中，也就是两个**相切**的地方取得极值。相切点的位置区别如下：\n",
    "\n",
    "1. 采用 L2 范数的时候，(EMB-1) 的解，也就是第一项等值线和 L2 范数等值线的**相切**点几乎处处会出现在某个象限，也就是 $w_1$ 和 $w_2$ 均非0；\n",
    "\n",
    "2. 采用 L1 范数的时候，(EMB-2) 的解，也就是第一项等值线和 L1 范数等值线的**相切（L1不是处处可微的，所以在尖点上实际是270度环绕的相交）**点几乎处处会出现在坐标轴上，也就是 $w_1$ 或者 $w_2$ 为0；\n",
    "\n",
    "所以我们说，**采用 L1 范数比 L2 范数更易于得到稀疏解。** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到 $\\mathbf{w}$ 取得稀疏解意味着初始的 d 个特征中仅有对应着 $\\mathbf{w}$ 的非零分量的特征才会出现在最终模型上。所以**求解 L1 范数正则化的结果是得到了仅采用一部分初始特征的模型；换言之，基于 L1 正则化的学习方法就是一种嵌入式的特征选择方法，其特征选择过程与学习训练过程融为一体。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求解 L1 正则化——近端梯度下降(PGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 正则化问题的求解可以使用近端梯度下降(Proximal Gradient Descent,简称PGD)。\n",
    "\n",
    "令 $\\nabla$ 表示微分算子，关于优化目标\n",
    "\n",
    "$$\\min_{\\mathbf{x}}f(\\mathbf{x}) +\\lambda ||\\mathbf{x}||_1 \\tag{PGD-0}$$ \n",
    "\n",
    "若 $f(\\mathbf{x})$ 可导，且 $\\nabla f$ 满足 L-Lipschitz 条件，即存在常数 $L > 0$ 使得\n",
    "\n",
    "$$||\\nabla f(\\mathbf{x}^\\prime) - \\nabla f(\\mathbf{x})||_2^2 \\leq L||\\mathbf{x}^\\prime - \\mathbf{x}||_2^2 \\quad (\\forall \\mathbf{x},\\mathbf{x}^\\prime)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注： L-Lipschitz 条件可以看成对函数二阶导函数的一个上界的控制：\n",
    " \n",
    " $$ \\Vert\\nabla(\\nabla f(\\mathbf{x}))\\Vert^2 = \\frac{||\\nabla f(\\mathbf{x}^\\prime) - \\nabla f(\\mathbf{x})||_2^2}{||\\mathbf{x}^\\prime - \\mathbf{x}||_2^2}\\leq L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step1. 我们先考虑式 (PGD-0) 第一项的最小值问题。**\n",
    "\n",
    "在 $\\mathbf{x}_k$ 附近可将 $f(\\mathbf{x})$ 通过二阶泰勒展式近似为(二阶导函数的上界为 L，因为泰勒展式是局部近似，所以可以由上界来代替)\n",
    "\n",
    "$$\\begin{split}\n",
    "\\hat{f}(\\mathbf{x}) &\\backsimeq f(\\mathbf{x}_k) + \\langle\\nabla f(\\mathbf{x}_k), \\mathbf{x} - \\mathbf{x}_k \\rangle+ \\frac{L}{2}||\\mathbf{x} - \\mathbf{x}_k||_2^2\\\\\n",
    "& =\\frac{L}{2}||\\mathbf{x} - \\mathbf{x}_k||_2^2 + \\langle\\nabla f(\\mathbf{x}_k), \\mathbf{x} - \\mathbf{x}_k \\rangle + \\frac{1}{2L}\\left(\\nabla f(\\mathbf{x}_k)\\right)^2 - \\frac{1}{2L}\\left(\\nabla f(\\mathbf{x}_k)\\right)^2 + f(\\mathbf{x}_k)\\\\\n",
    "& = \\frac{L}{2}\\left[ ||\\mathbf{x} - \\mathbf{x}_k||_2^2  + 2 \\langle \\frac{1}{L}\\nabla f(\\mathbf{x}_k), \\mathbf{x} - \\mathbf{x}_k \\rangle + \\left(\\frac{1}{L}\\nabla f(\\mathbf{x}_k)\\right)^2\\right] + f(\\mathbf{x}_k) - \\frac{1}{2L}\\left(\\nabla f(\\mathbf{x}_k)\\right)^2\\\\\n",
    "&= \\frac{L}{2}||\\mathbf{x} - \\mathbf{x}_k + \\frac{1}{L}\\nabla f(\\mathbf{x}_k)||_2^2 + \\phi(\\mathbf{x}_k)\n",
    "\\end{split}$$\n",
    "\n",
    "其中 $\\phi(\\mathbf{x}_k) = f(\\mathbf{x}_k) - \\frac{1}{2L}\\left(\\nabla f(\\mathbf{x}_k)\\right)^2$ 为与 $\\mathbf{x}$无关的常数，$\\langle\\cdot, \\cdot\\rangle$ 表示内积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式简写为\n",
    "\n",
    "\n",
    "$$\\hat{f}(\\mathbf{x}) \\backsimeq \\frac{L}{2}\\Vert\\mathbf{x} - (\\mathbf{x}_k -\\frac{1}{L}\\nabla f(\\mathbf{x}_k))\\Vert_2^2 + const \\tag{PGD-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，式 (PGD-1) 的最小值在$\\mathbf{x} = \\mathbf{x}_{k+1}$ 时获得(上式的最小值取决于右侧第一项的最小值)：\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\frac{1}{L}\\nabla f(\\mathbf{x}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，若通过梯度下降法对 $f(\\mathbf{x})$ 进行最小化，则每一步梯度下降迭代实际上等价于最小化二次函数 $\\hat{f}(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step2. 根据 $f(\\mathbf{x})$ 泰勒展式来求解 (PGD-0)**\n",
    "\n",
    "在 **step 1** 我们得到了 $f(\\mathbf{x})$ 的泰勒展式，则求解最小值的 (PGD-0) 在泰勒展式下所对应的迭代为\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\arg\\min_{\\mathbf{x}}\\frac{L}{2}\\Vert\\mathbf{x} - (\\mathbf{x}_k -\\frac{1}{L}\\nabla f(\\mathbf{x}_k))\\Vert_2^2 +\\lambda ||\\mathbf{x}||_1 \\tag{PGD-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于式 (PGD-2),可以先计算 $\\mathbf{z} = \\mathbf{x}_k - \\frac{1}{L}\\nabla f(\\mathbf{x}_k)$，然后求解\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\arg\\min_{\\mathbf{x}}\\frac{L}{2}\\Vert\\mathbf{x} - \\mathbf{z}\\Vert_2^2 +\\lambda ||\\mathbf{x}||_1 \\tag{PGD-3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记 $x_i,z_i$ 分别为$\\mathbf{x},\\mathbf{z}$ 的第 i 个分量，将式 (PGD-3) 展开可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}\n",
    "&\\frac{L}{2}\\Vert\\mathbf{x} - \\mathbf{z}\\Vert_2^2 +\\lambda ||\\mathbf{x}||_1\\\\\n",
    "=& \\sum_i \\frac{L}{2}(x_i-z_i)^2 + \\lambda|x_i|\\\\\n",
    "=& \\sum_i \\frac{L}{2}(x_i^2 - 2x_i z_i)+\\lambda x_i sign(x_i)+\\sum_i z_i^2\\\\\n",
    "=& \\sum_i \\frac{L}{2}x_i[x_i - 2z_i + \\frac{2\\lambda}{L}sign(x_i)]+const\\\\\n",
    "=& \\sum_i \\frac{L}{2}x_i\\left[x_i - 2\\left(z_i -\\frac{\\lambda}{L}sign(x_i)\\right)\\right]+const\\\\\n",
    "=& \\sum_i C_i + const\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $C_i = \\frac{L}{2}x_i\\left[x_i - 2\\left(z_i -\\frac{\\lambda}{L}sign(x_i)\\right)\\right]$ 是一个开口朝上的抛物线。\n",
    "\n",
    "注：此处的$sign(\\cdot)$ 是一个示性函数，在0处按照极限来定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它有两个根：\n",
    "\n",
    "$$x_1 = 0 \\quad and \\quad x_2 = 2\\left(z_i -\\frac{\\lambda}{L}sign(x_i)\\right)$$\n",
    "\n",
    "对应的最小值为\n",
    "\n",
    "$$x_\\min = \\frac{x_1+x_2}{2} = z_i -\\frac{\\lambda}{L}sign(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据 $x_1$ 和 $x_2$ 的大小关系，会得到\n",
    "\n",
    "$$x_{k+1}^i = \n",
    "\\left\\{\\begin{aligned}\n",
    "&z_i - \\lambda/L, & \\quad z_i > \\lambda/L;\\\\\n",
    "&0, &\\quad |z_i| \\leq \\lambda/L;\\\\\n",
    "&z_i + \\lambda/L, &\\quad z_i < - \\lambda/L.\n",
    "\\end{aligned}\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，通过 PGD 能使 LASSO 和其他基于 L1 范数最小化的方法得以快速求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sklearn.linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参数解释参考](https://blog.csdn.net/voidfaceless/article/details/61197999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sklearn.linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 稀疏表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn.decomposition.DictionaryLearning](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 压缩感知"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待补充"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
