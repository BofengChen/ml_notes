{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点到超平面的距离公式\n",
    "\n",
    "一个向量 $\\mathbf{x_1}$ 到超平面 $ \\mathbf{w}^T \\mathbf{x} + b = 0$ 的距离公式为\n",
    "\n",
    "$$d = \\frac{| \\mathbf{w}^T \\mathbf{x_1} + b|}{|| \\mathbf{w} ||}$$\n",
    "\n",
    "其中 $\\mathbf{w} = (w_1, w_2, ... ,w_n)^T$ 是超平面的法向量，b是常量,$||\\bullet||$ 是 $L_2$ 范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单证明如下：\n",
    "\n",
    "假设 $\\mathbf{x}_1$ 到 超平面的投影向量为 $\\mathbf{x}_0$， 则有 $ \\mathbf{w}^T \\mathbf{x}_0 + b = 0$ 。\n",
    "\n",
    "因为 $\\mathbf{x}_1 - \\mathbf{x}_0$ 与法向量 $\\mathbf{w}^T$ 平行，则有\n",
    "\n",
    "$$|\\mathbf{w}^T (\\mathbf{x}_1 - \\mathbf{x}_0)| = ||\\mathbf{w}|| |(\\mathbf{x}_1 - \\mathbf{x}_0)| = ||\\mathbf{w}||\\bullet d$$\n",
    "\n",
    "另一方面，\n",
    "\n",
    "$$\\mathbf{w}^T (\\mathbf{x}_1 - \\mathbf{x}_0) = \\mathbf{w}^T \\mathbf{x}_1 - \\mathbf{w}^T\\mathbf{x}_0 = \\mathbf{w}^T \\mathbf{x}_1 + b$$\n",
    "\n",
    "则点到超平面的距离 d 为\n",
    "$$d = \\frac{| \\mathbf{w}^T \\mathbf{x_1} + b|}{|| \\mathbf{w} ||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hoeffding Inequality](https://blog.csdn.net/liubai01/article/details/79947975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Jensen不等式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过一个凸函数上任意两点所作割线一定在这两点间的函数图象的下方，即：\n",
    "\n",
    "$$f(tx_1+(1-t)x_2)\\leq tf(x_1)+(1-t)f(x_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**概率密度函数的形式**\n",
    "\n",
    "假设$\\Omega$是实数轴上的可测子集，而 $f(x)$ 是非负函数，使得\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty f(x)\\,dx = 1$$\n",
    "\n",
    "$f$是一个概率密度函数。\n",
    "\n",
    "Jensen不等式变成以下关于凸积分的命题：\n",
    "\n",
    "若 g是任一实值可测函数，$\\phi$在 g 的值域中是凸函数，则\n",
    "\n",
    "$$\\varphi\\left(\\int_{-\\infty}^\\infty g(x)f(x)\\, dx\\right) \\le \\int_{-\\infty}^\\infty \\varphi(g(x)) f(x)\\, dx$$\n",
    "\n",
    "若$g(x)=x$，则这个形式的不等式简化为一个常用特例：\n",
    "\n",
    "$$\\varphi\\left(\\int_{-\\infty}^\\infty x\\, f(x)\\, dx\\right) \\le \\int_{-\\infty}^\\infty \\varphi(x)\\,f(x)\\, dx$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几何平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一组正数$\\{a_1,a_2,\\cdots,a_n\\}$对应的[**几何平均(Geometric mean)**](https://zh.wikipedia.org/wiki/%E5%87%A0%E4%BD%95%E5%B9%B3%E5%9D%87%E6%95%B0)为\n",
    "$$G(\\mathbf{a})=\\left(\\prod_{i=1}^n a_i\\right)^{\\frac{1}{n}}=\\sqrt[n]{a_1 a_2\\cdots a_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**几何平均与算术平均(arithmetic mean)的关系**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 $\\mathbf{A}$ 所包含的样本与 $\\mathbf{B}$ 之间对应的关系是：\n",
    "\n",
    "$$\\mathbf{A} = f(\\mathbf{B})$$\n",
    "其中 f 是广播函数，即满足\n",
    "$$A_i = f(B_i) = \\ln B_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则$\\mathbf{A}$ 的算术平均 $AM(\\mathbf{A})$ 与 $\\mathbf{B}$ 的几何平均 $GM(\\mathbf{B})$ 有如下关系：\n",
    "\n",
    "$$\\begin{split}\n",
    "AM(\\mathbf{A}) &= \\frac{A_1 + A_2 +\\cdots+A_n}{n}\\\\\n",
    "&= \\frac{\\ln A_1 + \\ln A_2 +\\cdots+\\ln A_n}{n}\\\\\n",
    "&=\\ln\\sqrt[n]{B_1\\cdots B_n}\\\\\n",
    "&=\\ln GM(\\mathbf{B})\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的推导我们知道：**一组样本取对数所得到的算术平均 等于 这组样本几何平均 的对数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几何平均的特点：\n",
    "* 几何平均相比算术平均，可以不受样本中比较大的值的影响，能够反映所有分量共同作用的平均结果；\n",
    "\n",
    "* 如果一个结果是由不同环节组成的，每个环节代表一个分量，那么当衡量所有环节对最终结果的平均影响时，用集合平均更合适。比如一项投资5年内的平均增长率，就适合通过几何平均来衡量；\n",
    "* 只有当所有分量相等时，几何平均与计算平均相等，其他情况都小于计算平均，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/GM.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadamard 乘积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于两个具有同样维数 $m\\times n$ 的矩阵 $\\mathbf{A}$ 和 $\\mathbf{B}$，我们定义它们的 [Hadamard](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))乘积为\n",
    "\n",
    "$$(\\mathbf{A}\\odot\\mathbf{B})_{ij} = (\\mathbf{A})_{ij}(\\mathbf{B})_{ij}$$\n",
    "\n",
    "如果两个矩阵的维数不一致，无法定义 Hadamard 乘积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `python` 中 可以通过 `numpy` 中的 `*`或 `np.mutiply(,)` 来实现 Hadamard 乘积；`np.dot` 实现一般意义上的矩阵相乘。如果是两个矩阵的 Hadamard 乘积，维数必须一致；如果是 向量与 $m\\times n$ 矩阵的 Hadamard 乘积，要求向量的维数必须为 $m\\times 1$ 或者 $1\\times n$,通过广播机制扩展成 $m\\times n$ 与矩阵进行 Hadamard 乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:47:12.199964Z",
     "start_time": "2019-04-19T06:47:12.188963Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(i.shape,m.shape)? (<ipython-input-1-14ecf89122f4>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-14ecf89122f4>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    print i.shape,m.shape\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(i.shape,m.shape)?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# hadamard 乘积\n",
    "x = np.array([2,2,2])#对应 1*3 的向量\n",
    "y = np.array([[2],[2]])#对应 3*1 的向量\n",
    "z = np.ones((2,3))\n",
    "m = np.ones((2,3))\n",
    "for i in [x,y,z,m]:\n",
    "    print i.shape,m.shape\n",
    "    if i is not m:\n",
    "        print i*m,'\\n',i*m == np.multiply(i,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:47:12.202964Z",
     "start_time": "2019-04-19T06:47:12.202Z"
    }
   },
   "outputs": [],
   "source": [
    "# 矩阵乘积 \n",
    "np.dot(np.ones((3,2)),m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 线性相关和生成子空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "下面我们讨论如下方程\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$$\n",
    "\n",
    "的解存在性和唯一性。其中 $\\mathbf{A}\\in R^{m\\times n}$是一个已知矩阵，$\\mathbf{b}\\in R^{m}$ 是一个已知向量，$\\mathbf{x}\\in R^{n}$ 是一个我们要求解的未知向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "为了分析方程有多少个解，我们可以将 $\\mathbf{A}$ 的列向量看作从**原点**（元素都是零的向量）出发的不同方向，确定有多少种方法可以到达向量 $\\mathbf{b}$。在这个观点下，向量 $\\mathbf{x}$ 的每个元素表示我们应该沿着这些方向走多远，即$x_i$ 表示我们需要沿着第 i 个向量的方向走多远：\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x}=\\sum_i x_i\\mathbf{A}_{:,i}$$\n",
    "\n",
    "称这种操作为**线性组合**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "一组向量的**生成子空间**是原始向量线性组合之后所能抵达的点的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***\n",
    "**存在解的充分必要条件**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "确定 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 是否有解，相当于确定向量 $\\mathbf{b}$ 是否在 $\\mathbf{A}$ 列向量的生成子空间中。这个特殊的生成子空间被称为 $\\mathbf{A}$ 的 **列空间(column space)**或者 $\\mathbf{A}$ 的 **值域(range)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "为了使方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 对任意向量 $\\mathbf{b}$ 都存在解，我们要求 $\\mathbf{A}$ 的列空间构成整个 $R^m$，这样才能确保任意的 $\\mathbf{b}$ 包含在 $\\mathbf{A}$ 的列空间里，也就是说任意的 $\\mathbf{b}$ 是可达的。由$\\mathbf{A}\\in R^{m\\times n}$ 可知， $\\mathbf{A}$的列空间构成整个 $R^m$的要求等价于说 $\\mathbf{A}$ 至少应该有 m 列，也就是说 $n \\geq m$。\n",
    "\n",
    "**不等式 $n \\geq m$ 只是解存在的必要不充分条件**，因为 $n \\geq m$ 并不能保证可以构成 $R^m$ 空间，换言之，不能保证 $\\mathbf{A}$ 的列空间存在 m 个 **线性无关** 的向量。\n",
    "\n",
    "**所以存在解的充分条件就是：** 矩阵 $\\mathbf{A}$ 必须包含至少一组 m 个线性无关的向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***\n",
    "**唯一解的条件**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "如果要保证对于每一个 $\\mathbf{b}$ 至多有一个解，我们需要确保矩阵至多有 m 个列向量。\n",
    "\n",
    "综上所述，**意味着这个矩阵必须是一个**方阵**，即 $m=n$；并且所有列向量都是线性无关的。**\n",
    "\n",
    "一个列向量线性相关的方阵被称为**奇异的(singular)**。\n",
    "\n",
    "一个矩阵 $\\mathbf{A}$ 不是方阵或者是一个奇异的方阵，该方程仍然有解，但是我们不能使用矩阵逆去求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迹运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迹运算返回的是矩阵对角线元素的和：\n",
    "\n",
    "$$Tr(\\mathbf{A}) = \\sum_{i}\\mathbf{A}_{i,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迹运算的性质：\n",
    "\n",
    "* 迹运算在转置运算下保持不变：\n",
    "$$Tr(\\mathbf{A})=Tr(\\mathbf{A}^T)$$\n",
    "\n",
    "* 迹运算在矩阵定义良好的循环乘积中保持不变：\n",
    "$$Tr(\\mathbf{A}\\mathbf{B}\\mathbf{C})=Tr(\\mathbf{C}\\mathbf{A}\\mathbf{B})=Tr(\\mathbf{B}\\mathbf{C}\\mathbf{A})$$\n",
    "更一般地，\n",
    "$$Tr\\left(\\prod_{i=1}^n\\mathbf{F}^{(i)}\\right)=Tr\\left(\\mathbf{F}^{(n)}\\prod_{i=1}^{n-1}\\mathbf{F}^{(i)}\\right)$$\n",
    "即使循环置换后矩阵乘积的矩阵形状从$m\\times m$ 变成 $n\\times n$，迹运算结果依然不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML&DL常用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [线性整流函数(Rectified Linear Unit, ReLU)](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0#%E5%B8%A6%E6%B3%84%E9%9C%B2%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数定义：\n",
    "\n",
    "$$g(z) = \\max\\{0,z\\}$$\n",
    "\n",
    "函数图像如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/reLU.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数特点：\n",
    "\n",
    "该函数在机器学习中 $g(-z)$ 会作为最原始的损失函数，如SVM中；在深度学习中会作为激活函数，被推荐用于大多数前馈神经网络激活函数。它用于将线性变换的输出将产生非线性变换。\n",
    "\n",
    "函数非常接近于线性，可以看做具有两个线性部分的分段线性函数。因为几乎线性的性质，保留了很多以此作为激活函数的线性模型易于使用基于梯度方法进行优化的属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic sigmoid 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义 **Logistic sigmoid 函数** 如下：\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "函数图像如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/logistic-sigmoid.jpg' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic sigmoid 函数特点**：\n",
    "\n",
    "1. Logistic sigmoid 函数通常用于产生 Bernoulli 分布中的参数 $\\phi$，因为它的范围是 $(0,1)$；\n",
    "\n",
    "\n",
    "2. **我们换一个视角来重新表达 sigmoid 函数时**：\n",
    "$$\\begin{split}\n",
    "&P(y=1|x) = \\frac{e^{1\\cdot x}}{e^{1\\cdot x}+e^{0\\cdot x}}=\\frac{1}{1+e^{-x}}=\\sigma(x)\\\\\n",
    "&P(y=0|x) = \\frac{e^{0\\cdot x}}{e^{1\\cdot x}+e^{0\\cdot x}}=\\frac{1}{1+e^x}=\\sigma(-x)\n",
    "\\end{split}$$\n",
    "我们会发现 sigmoid 函数可以看做服从 Bernoulli 分布的 $y=\\{0,1\\}$ 时的关于 x 的条件概率。分母的部分属于归一化部分，概率分布对应的构造函数为 $e^{yx}$, 我们可以统一表示服从Bernoulli 分布的概率函数为：\n",
    "$$P(y=y_i|x)=\\sigma\\left((2y_i-1)x\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. 逻辑回归模型中也用到了这个函数以及其逆函数(对数几率函数)：\n",
    "$$x =\\sigma^{-1}(y)= \\ln \\frac{y}{1-y}$$\n",
    "\n",
    "\n",
    "4. sigmoid 函数的导数性质：\n",
    "$$\\frac{d\\sigma}{dx}= \\sigma(1-\\sigma)$$\n",
    "\n",
    "\n",
    "5. 因为 sigmoid 函数在无穷远处有如下性质：\n",
    "$$\\begin{split}&\\sigma(x)\\rightarrow 0\\;(x\\rightarrow -\\infty)\\\\\n",
    "&\\sigma(x)\\rightarrow 1\\;(x\\rightarrow +\\infty)\\end{split}$$\n",
    "所以有 $\\frac{d\\sigma}{dx}\\rightarrow 0 \\; (|x|\\rightarrow \\infty)$\n",
    "\n",
    "\n",
    "6. 如图所示以及上述第四点关于导数在正负无穷远处为零的性质可知：\n",
    "\n",
    "    * **sigmoid 函数在变量趋于正负无穷时(实际操作中就是指取值的绝对值比较大时)会出现饱和(saturate)现象(导数等于0)，意味着函数会变得很平，并且对输入的微小改变不敏感。**\n",
    "    \n",
    "    * **函数在接近 0 点附近导数值比较大，也就是对数值的变动比较敏感，而远离 0 点时会饱和性越来越强(导数越来趋向于0)，所以更适合做输出单元，当然要配合抑制饱和的代价函数来使用，并不太适合做隐藏单元的激活函数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 Softplus 函数如下：\n",
    "\n",
    "$$\\zeta(\\mathbf{x}) = \\ln(1+e^{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softplus 函数名来源于它是线性整流函数的平滑形式,下面是其图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/softplus.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softplus 函数特点**：\n",
    "\n",
    "1. Softplus 函数通常用于产生 正态分布的参数 $\\beta,\\alpha$，因为它的范围是 $(0,\\infty)$；\n",
    "\n",
    "\n",
    "2. Softplus 函数的导数性质：\n",
    "$$\\frac{d\\zeta}{dx}= \\sigma$$\n",
    "也就是说 Softplus 函数的导数恰好是sigmoid 函数;\n",
    "\n",
    "\n",
    "3. 由上述第二点的导数性质以及sigmoid 函数在负无穷远处趋于0，所以有\n",
    "$$\\frac{d\\zeta}{dx}\\rightarrow 0\\;(x\\rightarrow -\\infty)$$\n",
    "\n",
    "\n",
    "4. Softplus 函数在 $x\\rightarrow -\\infty$ 处有如下性质：\n",
    "$$\\begin{split}&\\zeta(x)\\rightarrow 0\\;(x\\rightarrow -\\infty)\\end{split}$$\n",
    "\n",
    "\n",
    "5. 因为：\n",
    "$$\\lim_{x\\rightarrow +\\infty}\\frac{x}{\\zeta(x)} \\overset{Lhopital}{=}\\lim_{x\\rightarrow +\\infty}\\frac{1}{\\sigma(x)}=1$$\n",
    "所以Softplus 函数在 $x\\rightarrow +\\infty$ 处有\n",
    "$$\\zeta(x)\\sim x\\;(x\\rightarrow +\\infty)$$\n",
    "也就是说，在变量趋于无穷远时 Softplus 函数等价于 $y=x$ 函数，这一点从图示上也能看出来。\n",
    "\n",
    "\n",
    "\n",
    "6. 如图所示以及上述第三点关于导数在负无穷远处为零的性质可知，**Softplus 函数在变量趋于负无穷时(实际操作中就是指取负值的绝对值比较大时)会出现饱和(saturate)现象，意味着函数会变得很平，并且对输入的微小改变不敏感。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对向量 $\\mathbf{x}$,我们定义它的**softmax 函数**如下：\n",
    "\n",
    "$$\\mathbf{y} = softmax(\\mathbf{x})$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$y_i = \\frac{\\exp{(x_i)}}{\\sum_{j=1}^n \\exp{(x_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以很明显看出， Softmax 函数是 sigmoid 函数的扩展，相应地，可以视为定义了 Multinoulli 分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 函数的数值方法稳定的版本\n",
    "\n",
    "$$softmax(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$ \n",
    "\n",
    "其中\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{x} - \\max_k x_k$$\n",
    "\n",
    "上述的运算是广播机制,这种线性平移映射不会影响或者改变 $softmax(\\mathbf{x})$ 的输出值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**关于饱和性质**：\n",
    "\n",
    "首先我们变换下数值稳定版本的表达式：\n",
    "\n",
    "$$\\begin{split}\n",
    "softmax(\\mathbf{z})_i &= \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_i}}\\\\\n",
    "\\\\\n",
    "&= \\frac{exp(x_i-\\max_k x_k)}{\\sum_{j=1}^n exp(x_j-\\max_k x_k)}\\\\\n",
    "\\\\\n",
    "&=\\frac{exp(x_i-\\max_k x_k)}{1+\\sum_{j\\neq k} exp(x_j-\\max_k x_k)}\n",
    "\\end{split}$$ \n",
    "\n",
    "由上式我们可知\n",
    "\n",
    "* 当 $x_i = \\max_k x_k \\; \\& \\; x_i\\gg x_j \\; (i\\neq j)$ 时, $softmax(\\mathbf{z})_i\\rightarrow 1$;\n",
    "* 当 $x_i \\neq \\max_k x_k \\; \\& \\; x_i\\ll \\max_k x_k$ 时, $softmax(\\mathbf{z})_i\\rightarrow 0$。\n",
    "\n",
    "这种饱和性质属于上面我们介绍的 sigmoid 函数饱和性质的一般化体现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双曲正切函数定义为双曲正弦与双曲余弦的比值，如下：\n",
    "\n",
    "$$\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)}=\\frac{e^x - e^{-x}}{e^x + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像如下：\n",
    "\n",
    "\n",
    "<img src='figure/tanh.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双曲正切函数的性质如下：\n",
    "\n",
    "1. $\\tanh(x)$ 是奇函数，即 $\\tanh(x)=\\tanh(-x)$;\n",
    "\n",
    "\n",
    "2. 双曲正切函数可以看做是符号函数 sign(x) 的平滑形式；\n",
    "\n",
    "\n",
    "3. $\\tanh(x)$ 函数在正负无穷远处的极限为 $+1,-1$；在零点附近近似于 $y=x$;\n",
    "\n",
    "\n",
    "4. $\\tanh(x)=2\\sigma(2x)-1$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均值的标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差有如下性质：\n",
    "\n",
    "$$\\begin{split}\n",
    "Var(x) \n",
    "&= E\\left[(x-E(x))^2\\right]\\\\\n",
    "&=E\\left[x^2-2xE(x) +E^2(x)\\right]\\\\\n",
    "&= E(x^2) - 2E^2(x)+E^2(x)\\\\\n",
    "&=E(x^2) - E^2(x)\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以：\n",
    "\n",
    "$$Var(ax) = E(a^2x^2) - E^2(ax) = a^2(E(x^2) - E^2(x)) = a^2 Var(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集上随机抽取 m 个样本$\\{x_i|1\\leq i\\leq m\\}$，记这些样本在真实分布 $p_{data}$ 上的标准差为 $\\sigma$，记 m 个样本的均值为：\n",
    "\n",
    "$$\\hat{\\mu}_m = \\frac{1}{m}\\sum_{i=1}^m x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记这 m 个样本均值所对应的标准差如下：\n",
    "\n",
    "$$\\begin{split}SE(\\hat{\\mu}_m) \n",
    "&= \\sqrt{Var\\left[\\frac{1}{m}\\sum_{i=1}^m x_i\\right]}\\\\\n",
    "&= \\sqrt{\\frac{1}{m^2}\\sum_{i=1}^m Var(x_i)}\\\\\n",
    "&= \\sqrt{\\frac{1}{m^2}\\sum_{i=1}^m \\sigma^2}\\\\\n",
    "&=\\frac{\\sigma}{\\sqrt{m}}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即：\n",
    "\n",
    "$$SE(\\hat{\\mu}_m) = \\frac{\\sigma}{\\sqrt{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均值的标准差在机器学习实验中非常有用，通常用测试集样本的误差期望来估计泛化误差，而测试集中样本的数量决定了估计的精确度。\n",
    "\n",
    "中心极限定理告诉我们均值会接近于一个高斯分布，我们可以用标准差计算出真实期望落在选定区间的概率。比如以均值 $\\hat{\\mu}_m$ 为中心的 95% 置信区间为\n",
    "\n",
    "$$(\\hat{\\mu}_m - 1.96 SE(\\hat{\\mu}_m), \\hat{\\mu}_m + 1.96 SE(\\hat{\\mu}_m))$$\n",
    "\n",
    "以上区间是基于均值 $\\hat{\\mu}_m$ 和 方差 $SE(\\hat{\\mu}_m)$ 的高斯分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习实验中，我们通常说算法 A 比算法 B 好，是指：\n",
    "\n",
    "$$ 算法 A 的误差的 95\\% 置信区间的上界 < 算法 B 误差的 95\\% 置信区间的下界 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数值计算\n",
    "\n",
    "部分内容来自于冯果忱主编的《数值分析(上册)》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius 范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于矩阵 $\\mathbf{A}$,我们定义其 Frobenius 范数为\n",
    "\n",
    "$$||\\mathbf{A}||_F = \\sqrt{\\sum_{i,j}A_{i,j}^2} = \\sqrt{Tr(\\mathbf{A}\\mathbf{A}^T)}$$\n",
    "\n",
    "类似于向量的 $L^2$ 范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从属范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何矩阵 $\\mathbf{A}\\in R^{m\\times n}$，定义\n",
    "\n",
    "$$||\\mathbf{A}|| = \\max_{||\\mathbf{x}||=1}||\\mathbf{A}\\mathbf{x}||$$\n",
    "\n",
    "为矩阵 $\\mathbf{A}$ 的范数。如此得到的矩阵范数 $||\\cdot||$ 称为向量范数 $||\\cdot||$ 的**从属范数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 谱范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(向量)2范数**的从属范数为 \n",
    "\n",
    "$$||\\mathbf{A}||_2 = \\max_{||\\mathbf{x}||=1}||\\mathbf{A}\\mathbf{x}||_2 = \\sqrt{\\lambda_\\max}\\tag{Fanshu-1}$$\n",
    "\n",
    "其中 $\\lambda_\\max$ 是矩阵 $\\mathbf{A}^T\\mathbf{A}$ 的最大特征值。\n",
    "\n",
    "**注：证明见冯果忱主编的《数值分析(上册)》的 P25.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意矩阵 $\\mathbf{B}\\in R^{n\\times n}$ 的特征值的按模最大值 $\\lambda_\\max(\\mathbf{B})$，称为 $\\mathbf{B}$ 的**谱半径**，记为 $\\rho(\\mathbf{B})$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据谱半径的定义，我们可以重新将(Fanshu-1)记为\n",
    "\n",
    "$$||\\mathbf{A}||_2 = \\sqrt{\\rho(\\mathbf{A}^T\\mathbf{A})}\\tag{Fanshu-2}$$\n",
    "\n",
    "所以 $||\\mathbf{A}||_2$ 也叫做 $\\mathbf{A}$ 的**谱范数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别地，**当$\\mathbf{A}\\in R^{n\\times n}$ 为对称矩阵时($\\mathbf{A}^T\\mathbf{A} = \\mathbf{A}^2$)**,记 $\\mathbf{A}^T\\mathbf{A}$ 的最大特征值为 $\\lambda_\\max$，$\\mathbf{A}$ 最大特征值为 $\\lambda_{A\\max}$,则有 \n",
    "\n",
    "$$\\lambda_\\max = \\lambda_{A\\max}^2,\\quad \\rho(\\mathbf{A}^T\\mathbf{A}) = (\\rho(\\mathbf{A}))^2$$\n",
    "\n",
    "从而\n",
    "\n",
    "$$\\begin{split}\n",
    "||\\mathbf{A}||_2 &= \\sqrt{\\rho(\\mathbf{A}^T\\mathbf{A})} =\\sqrt{\\lambda_\\max}\\\\\n",
    "& =\\rho(\\mathbf{A}) = \\lambda_{A\\max}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵的条件数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性方程组\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$$\n",
    "\n",
    "假设它的解存在且唯一，即 $\\mathbf{A}^{-1}$ 存在，下面讨论右端项的误差对解的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 $\\mathbf{x}$ 表示方程组的精确解，假设方程组右端存在误差 $\\delta \\mathbf{b}$，对应引起的解的误差为 $\\delta \\mathbf{x}$,则我们有\n",
    "\n",
    "$$\\mathbf{A}(\\mathbf{x}+\\delta \\mathbf{x})=\\mathbf{b} + \\delta \\mathbf{b}$$\n",
    "\n",
    "由此得到误差方程\n",
    "\n",
    "$$\\mathbf{A}\\delta \\mathbf{x}=\\delta \\mathbf{b}$$\n",
    "\n",
    "从而\n",
    "\n",
    "$$||\\delta \\mathbf{x}||\\leq ||\\mathbf{A}^{-1}||\\;||\\delta \\mathbf{b}|| \\tag{Cond-1}$$\n",
    "\n",
    "注意\n",
    "\n",
    "$$||\\mathbf{b}||\\leq ||\\mathbf{A}||\\;||\\mathbf{x}||$$\n",
    "\n",
    "于是由 (Cond-1) 可得到\n",
    "\n",
    "$$\\frac{||\\delta \\mathbf{x}||}{||\\mathbf{x}||}\\leq ||\\mathbf{A}||\\;||\\mathbf{A}^{-1}||\\; ||\\frac{\\delta \\mathbf{b}}{\\mathbf{b}}||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式中的系数 $||\\mathbf{A}||\\;||\\mathbf{A}^{-1}||$ 决定了对误差的敏感程度，称为 $||\\mathbf{A}||$ 的**条件数**，记为\n",
    "\n",
    "$$Cond(\\mathbf{A}) = ||\\mathbf{A}||\\cdot||\\mathbf{A}^{-1}||$$\n",
    "\n",
    "条件数的大小和所取的**范数**有关。当该数很大时，矩阵求逆对输入的误差特别敏感。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设 $\\mathbf{A}$ 的特征值为 $|\\lambda_1|\\geq |\\lambda_2|\\geq \\cdots \\geq |\\lambda_n|$>0，当矩阵 $\\mathbf{A}$ 为**对称矩阵**, $\\mathbf{A}$ 和 $\\mathbf{A}^{-1}$ 对应的 **谱范数** 如下：\n",
    "\n",
    "$$||\\mathbf{A}||_2 = \\rho(\\mathbf{A}) = |\\lambda_1| \\quad ||\\mathbf{A}^{-1}||_2 = \\rho(\\mathbf{A}^{-1}) = \\frac{1}{|\\lambda_n|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则对称矩阵 $\\mathbf{A}$ 所对应的条件数为\n",
    "\n",
    "$$Cond(\\mathbf{A}) = ||\\mathbf{A}||_2\\cdot||\\mathbf{A}^{-1}||_2 = \\left|\\frac{\\lambda_1}{\\lambda_n}\\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于方阵，利用特征值和特征向量可以刻画矩阵的结构，在长方形矩阵情形这些方法不适用，推广的特征值即奇异值分解理论改善了这个状况。利用奇异值和奇异向量不仅可以刻画矩阵本身，还可以进一步刻画线性代数方程组解的结构，是构造性地研究线性代数问题的有力工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**定理** 设 $\\mathbf{A}\\in R^{m\\times n}$, 则存在正交矩阵 $\\mathbf{U} = (\\mathbf{u}_1, \\cdots,\\mathbf{u}_m)\\in R^{m\\times m},\\mathbf{V} = (\\mathbf{v}_1, \\cdots,\\mathbf{v}_n)\\in R^{n\\times n}$ 使\n",
    "\n",
    "$$\\mathbf{U}^T \\mathbf{A}\\mathbf{V}=\n",
    "\\left(                 \n",
    "\\begin{array}{cc}   \n",
    "\\mathbf{\\sum} & \\mathbf{O}\\\\  \n",
    "\\mathbf{O} & \\mathbf{O}\\\\  \n",
    "\\end{array} \n",
    "\\right)$$\n",
    "\n",
    "其中 $\\mathbf{\\sum} = diag(\\sigma_1, \\cdots, \\sigma_r)$，并且 $\\sigma_1\\geq \\sigma_2\\geq\\cdots\\geq \\sigma_r >0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/SVD.jpg' width='500' height='400' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "我们记\n",
    "\n",
    "$$\\mathbf{D} = \\left(                 \n",
    "\\begin{array}{cc}   \n",
    "\\mathbf{\\sum} & \\mathbf{O}\\\\  \n",
    "\\mathbf{O} & \\mathbf{O}\\\\  \n",
    "\\end{array} \n",
    "\\right)$$\n",
    "\n",
    "则有 $\\mathbf{U}^T \\mathbf{A}\\mathbf{V}= \\mathbf{D}$, 由 $\\mathbf{U},\\mathbf{V}$ 的正交性质可知\n",
    "\n",
    "$$\\mathbf{A}= \\mathbf{U}\\mathbf{D}\\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**推论：** $\\mathbf{V}$ 的列向量是矩阵 $\\mathbf{A}^T\\mathbf{A}$ 的特征向量；$\\mathbf{U}$ 的列向量是矩阵 $\\mathbf{A}\\mathbf{A}^T$ 的特征向量，即\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{V}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{V} = diag(\\sigma_1^2,\\cdots,\\sigma_r^2,0,\\cdots,0),\\\\\n",
    "\\mathbf{U}^T\\mathbf{A}\\mathbf{A}^T\\mathbf{U} = diag(\\sigma_1^2,\\cdots,\\sigma_r^2,0,\\cdots,0),\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们简单证明关于 $\\mathbf{V}$ 的结论，关于 $\\mathbf{U}$ 类似。\n",
    "\n",
    "由 $\\mathbf{U}^T \\mathbf{A}\\mathbf{V}= \\mathbf{D}$ 可知\n",
    "\n",
    "$$\\begin{split}\n",
    "diag(\\sigma_1^2,\\cdots,\\sigma_r^2,0,\\cdots,0) &= \\mathbf{D}^T\\mathbf{D}\\\\\n",
    "&=(\\mathbf{U}^T \\mathbf{A}\\mathbf{V})^T(\\mathbf{U}^T \\mathbf{A}\\mathbf{V})\\\\\n",
    "&=\\mathbf{V}^T\\mathbf{A}^T\\mathbf{U}\\mathbf{U}^T \\mathbf{A}\\mathbf{V}\\\\\n",
    "&=\\mathbf{V}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{V}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "综上所述：\n",
    "* 对角矩阵 $\\mathbf{D}$ 对角线上的元素称为矩阵 $\\mathbf{A}$ 的**奇异值**；\n",
    "* 矩阵 $\\mathbf{U}$ 的列向量称为**左奇异向量**，其列向量为矩阵 $\\mathbf{A}^T\\mathbf{A}$ 的特征向量；\n",
    "* 矩阵 $\\mathbf{V}$ 的列向量称为**右奇异向量**，其列向量为矩阵 $\\mathbf{A}\\mathbf{A}^T$ 的特征向量；\n",
    "* $\\mathbf{A}$ 的非零奇异值为矩阵 $\\mathbf{A}^T\\mathbf{A}$，同时也是 $\\mathbf{A}\\mathbf{A}^T$ 的特征值的平方根。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[奇异值分解(SVD) --- 线性变换几何意义](http://blog.sciencenet.cn/home.php?mod=space&uid=696950&do=blog&quickforward=1&id=699380)\n",
    "\n",
    "[奇异值分解(SVD) --- 几何意义](http://blog.sciencenet.cn/blog-696950-699432.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上溢和下溢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们介绍在进行数值计算时会遇到的三种误差类型：\n",
    "\n",
    "1. **舍入误差**：一个数 $x^\\ast$ 与它经过“舍入”或“切断”产生近似数 x 之间的误差称为 **舍入误差**。\n",
    "\n",
    "\n",
    "2. **截断误差**：许多数学运算（比如微分、积分、无穷级数求和等）是通过极限过程定义的，但在计算机上只能完成有限次的运算。用有限过程近似无穷过程，表现为对无穷过程的截断，由此产生的误差称为 **舍入误差**。\n",
    "\n",
    "\n",
    "3. **传播误差**：许多数值方法是用递归公式的形式给出的，初始数据往往具有误差，每一步计算也要产生舍入误差。这些误差参加运算，要影响后面的计算结果，即这些误差要向后传播，由此产生的误差称为**传播误差**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实数在计算机上的表示是通过有限数量的位模式来进行表示，所以会引入不可避免的近似误差。许多情况下，仅仅是舍入误差。\n",
    "\n",
    "一种极具毁灭性的舍入误差是**下溢（underflow）**：当接近零的数被四舍五入为零时会发生下溢。另一种极具破坏力的数值错误形式是**上溢**：当大量级的数被近似为 $\\infty$ 或 $-\\infty$ 时会发生上溢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个必须对上溢和下溢进行稳定的例子是 **softmax 函数**：\n",
    "\n",
    "$$softmax(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当所有的 $x_i$ 取值为 c 时，理论上每个输出都应该是 $\\frac{1}{c}$，由\n",
    "$$\\begin{split}\n",
    "& \\lim_{c\\rightarrow +\\infty}e^c = +\\infty\\\\\n",
    "& \\lim_{c\\rightarrow -\\infty}e^c = 0\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "可知，当 c 量级很大时会发生上溢；c 是很小的负数时会发生下溢。softmax 函数的分母会变成 0 。这两个数值计算上不稳定的问题可以通过定义 \n",
    "\n",
    "$$softmax(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$ \n",
    "\n",
    "其中\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{x} - \\max_i x_i$$\n",
    "\n",
    "上述的运算是广播机制,这种线性平移映射不会影响或者改变 $softmax(\\mathbf{x})$ 的输出值。\n",
    "\n",
    "减去 $\\max_i x_i$ 使得 \n",
    "\n",
    "$$e^{z_i} \\leq 1$$\n",
    "\n",
    "所以不会发生上溢，同时分母中至少有一个值为 1 的项，不会使得分母为 0 从而发生数值下溢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 度量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在谈信息熵之前，我们先提一个概念：**信息量(专业术语是：自信息), 信息量等于传输该信息所用的代价**，也可以说是，解决这个问题所需要的信息量大小。我们日常经常会说，这个人长篇大论没有什么信息量，可以视为我们对这个人说这段话所包含的信息的度量。信息量应该至少满足三个条件：\n",
    "* 非负性：一条额外信息传递所带来的累积信息量不应该小于传递这条额外信息之前，所以这条额外信息的信息量（即累积信息量的差）应该是非负的；\n",
    "\n",
    "\n",
    "* 可加性：针对同一件事，两个人传递的两条独立信息所包含的信息量是可加的；\n",
    "\n",
    "\n",
    "* **与事件本身发生的概率成反比**： 对于一件事，什么信息包含的信息量大呢？**发生概率低的信息所携带的信息量大**，如果一个人告诉你，太阳从东方升起（百分百发生的事情，携带的信息量为0）。\n",
    "\n",
    "总的来说，一条信息的信息量和它的不确定性有直接关系。信息量的数学定义为：$f(x)=-log_{2}P(x)$，其中 P(x) 为该事件发生的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息熵（香农熵）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义一个事件 x 的自信息（信息量）为\n",
    "\n",
    "$$I(x) = -\\log P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用**香农熵(信息熵)**来对整个概率分布中的不确定性总量进行量化：\n",
    "\n",
    "$$H(x)=-E_{x\\sim P}\\log P(x)$$\n",
    "\n",
    "也记为 $H(P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习中的决策树选择分裂节点的信息增益概念就是以此为基础来定义的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对于同一个随机变量 x 有两个单独的概率分布 $P(x)$ 和 $Q(x)$，可以使用 **KL散度** 或者 **相对熵(relative entropy)**来衡量这两个分布的差异：\n",
    "\n",
    "$$D_{KL}(P||Q) = E_{x\\sim P}\\left[\\log\\frac{P(x)}{Q(x)}\\right]=E_{x\\sim P}[\\log P(x)-\\log Q(x)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL 散度的性质：\n",
    "\n",
    "* KL 散度是不对称的；\n",
    "* KL 散度是非负的；\n",
    "* KL 散度为0，当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的。\n",
    "\n",
    "因为 KL 散度是非负的，并且衡量的是两个分布之间的差异，经常被用作度量两个分布之间的“距离”；然而因为 KL 散度不对称的性质，它并不是真的距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KL散度非负的证明**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 Jensen 不等式可知：\n",
    "\n",
    "若 f 是一个概率密度函数，g是任一实值可测函数，$\\phi$在 g 的值域中是凸函数，则\n",
    "\n",
    "$$\\varphi\\left(\\int_{-\\infty}^\\infty g(x)f(x)\\, dx\\right) \\le \\int_{-\\infty}^\\infty \\varphi(g(x)) f(x)\\, dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令 $\\varphi(x)=-\\ln x$，是一个严格上凸函数，则有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}\n",
    "D_{KL}(P||Q)&=\\int_{-\\infty}^\\infty \\ln(\\frac{P(x)}{Q(x)}) P(x)\\, dx\\\\\n",
    "&=\\int_{-\\infty}^\\infty -\\ln(\\frac{Q(x)}{P(x)}) P(x)\\, dx\\\\\n",
    "&\\geq -\\ln\\left(\\int_{-\\infty}^\\infty \\frac{Q(x)}{P(x)}P(x)\\, dx\\right)=-\\ln 1 = 0\n",
    "\\end{split}$$\n",
    "\n",
    "当且仅当 $P(x)=Q(x)$ 时等号完全成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [交叉熵](https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和 KL 散度密切联系的量就是**交叉熵(cross-entropy)**:\n",
    "\n",
    "$$ H(P,Q)=H(P)+D_{KL}(P||Q) = -E_{x\\sim P}\\log Q(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [最大似然估计（maximum likelihood estimation，缩写为MLE）](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **二分类问题对应的定义**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令 $P(Y=1|X=x) = P(x;\\theta)$, 其中 P 是以 $\\theta$ 为参数的函数。假设样本 $x_i$ 之间相互独立，则似然函数(likelihood function)为\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^m P(Y=y_i|X=x_i) = \\prod_{i=1}^m P(x_i;\\theta)^{y_i}(1-P(x_i;\\theta))^{(1- y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对 $L(\\theta)$取对数，则得到**对数似然函数**（log-likelihood function）如下：\n",
    "\n",
    "$$\\ell(\\theta) = \\ln L(\\theta) = \\sum_{i=1}^{m} y_i \\ln P(x_i;\\theta) + \\sum_{i=1}^{m} (1-y_i) \\ln (1-P(x_i;\\theta))$$\n",
    "\n",
    "称\n",
    "\n",
    "$$ \\theta^{\\star} = \\arg\\max_{\\theta}\\ell(\\theta) = -\\arg\\min_{\\theta}\\ell(\\theta)$$\n",
    "\n",
    "为**最大化对数似然估计**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一般意义下的最大似然估计**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑一组含有 m 个样本的数据集 $\\mathbf{X} = \\{\\mathbf{x}_1,\\mathbf{x}_2, \\cdots,\\mathbf{x}_m\\}$，由未知的真实数据分布 $p_{data}(\\mathbf{x})$ 生成。\n",
    "\n",
    "令 $p_{model}(\\mathbf{x};\\mathbf{\\theta})$ 是一族由 $\\mathbf{\\theta}$ 确定在相同空间上的概率分布。换言之，$p_{model}(\\mathbf{x};\\mathbf{\\theta})$ 是 模型或者神经网络模型对 $p_{data}(\\mathbf{x})$ 的逼近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则 $\\mathbf{\\theta}$ 的最大似然估计被定义为\n",
    "\n",
    "$$\\mathbf{\\theta}_{ML} = \\arg\\max_{\\mathbf{\\theta}}p_{model}(\\mathbf{X};\\mathbf{\\theta})=\\arg\\max_{\\mathbf{\\theta}}\\prod_{i=1}^m p_{model}(\\mathbf{x}_i;\\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多个数值的乘积容易导致数值下溢，我们发现似然对数不会改变 $\\arg\\max$，我们将乘积通过对数函数转换为求和形式：\n",
    "\n",
    "$$\\mathbf{\\theta}_{ML} = \\arg\\max_{\\mathbf{\\theta}}\\sum_{i=1}^{m}\\log p_{model}(\\mathbf{x}_i;\\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为缩放代价函数不会改变 $\\arg\\max$，我们可以除以 m 得到和训练数据经验分布 $\\hat{p}_{data}(\\mathbf{x})$ 相关的期望作为准则：\n",
    "\n",
    "$$\\mathbf{\\theta}_{ML} = \\arg\\max_{\\mathbf{\\theta}}E_{\\mathbf{x}\\sim \\hat{p}_{data}}\\log p_{model}(\\mathbf{x};\\mathbf{\\theta})$$\n",
    "\n",
    "注：[大数定律](https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B)可以支持以上转换的成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最优化意义下三者的等价性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由交叉熵的定义可知\n",
    "\n",
    "$$\\min_{Q}H(P,Q) = \\min_{Q}D_{KL}(P||Q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种解释最大似然估计的观点是，将它看做**最小化训练集上的经验分布 $\\hat{p}_{data}$ 和模型分布之间的差异(也就是模型分布尽可能和经验分布 $\\hat{p}_{data}$ 相匹配**)，两者之间的差异可以通过 KL 散度度量：\n",
    "\n",
    "$$D_{KL}(\\hat{p}_{data}||p_{model}) =E_{x\\sim \\hat{p}_{data}}\\;[\\log \\hat{p}_{data}(\\mathbf{x})-\\log p_{model}(\\mathbf{x})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "等式右侧第一项不涉及模型，意味着训练模型最小化 KL 散度时，只需要最小化\n",
    "\n",
    "$$-E_{x\\sim \\hat{p}_{data}}\\;\\log p_{model}(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有前面可知，最大似然为\n",
    "\n",
    "$$\\mathbf{\\theta}_{ML} = \\arg\\max_{\\mathbf{\\theta}}E_{\\mathbf{x}\\sim \\hat{p}_{data}}\\log p_{model}\n",
    "(\\mathbf{x};\\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以**最大化对数似然等价于最小化 KL 散度，也等价于最小化交叉熵。**即：\n",
    "\n",
    "$$\\begin{split}\\mathbf{\\theta}_{ML} =\\min_{p_{model}}\\; D_{KL}(\\hat{p}_{data}||p_{model})=\\min_{p_{model}}\\; H(\\hat{p}_{data},p_{model})\n",
    "\\end{split}$$\n",
    "\n",
    "在最优化的意义下三者是等价的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE: 均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于样例集合$D = \\{ (\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ... , (\\mathbf{x}_n, y_n)\\}$,通过“均方误差”（mean squared error）来进行性能度量：\n",
    "\n",
    "$$E(f;D) = \\frac{1}{m}\\sum_{i=1}^m (f(\\mathbf{x}_i) - y_i)^2$$\n",
    "\n",
    "“均方误差”的几何意义是学习器与真实样本之间的 $L^2$ 范数。**基于 MSE 最小化来求解 f 的方法称为“最小二乘法”**。具体求解过程等同于为凸函数求解最小值，不赘述。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "473px",
    "left": "114px",
    "top": "141px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
