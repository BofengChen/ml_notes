{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习(ensemble learning)通过 **<个体学习器集，策略>** 来进行学习。其中，**个体学习器**一般是一个现有的学习算法通过训练数据产生。按照个体学习器是否同类可以将集成学习分为两类：\n",
    "* 如果个体学习器**只包含同类型的学习器，我们称这样的集成是“同质的”**，通过 **<基学习器集，策略>**（称个体学习器为基学习器）进行学习，比如“决策树集成”里的个体学习器都是决策树；\n",
    "\n",
    "\n",
    "* 如果个体学习器包含**不同类型的学习器，我们称这样的集成是“异质的”**，通过 **<组件学习器集，策略>**（称个体学习器为组件学习器）进行学习，比如同时包含决策树和神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习的示意图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/ensemble_learning.png\" width=\"400\" hegiht=\"300\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**集成学习通常可以获得比单一学习器更显著卓越的泛化性能，这对“弱学习器”（泛化性能稍微好于50%的学习器）效果尤为明显。**因此很多集成学习的理论研究都是针对弱学习器来进行的，但实践上出于种种考虑，例如希望使用较少的个体学习器，往往会使用比较强的学习器。\n",
    "\n",
    "要获得好的集成，<span class=\"mark\">个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，并且要有“多样性”，但两者一般情况下是冲突的</span>，如何平衡两者之间的冲突也是集成学习研究的核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据个体学习器序列的生成方式，目前集成学习方法分为两类：\n",
    "\n",
    "1. **降低偏差（防止欠拟合）**：个体学习器之间存在强依赖关系、必须**串行**生成的序列化方法，代表集成是 _**Boosting**_；\n",
    "2. **降低方差（防止过拟合）**：个体学习器之间不存在强依赖关系、可同时生成的**并行化方法**，代表集成是_**Bagging, Random Forest**_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习对弱学习器泛化能力的提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**假设：基学习器的误差相互独立。（现实任务中，显然不可能相互独立）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑二分类问题$y\\in\\{-1, +1\\}$和真实函数 f，假设基分类器的错误率为$\\epsilon$，即对每个基分类器$h_i$，有\n",
    "\n",
    "$$P(h_i(\\mathbf{x})\\neq f(\\mathbf{x})) = \\epsilon$$\n",
    "\n",
    "假设分类器为\n",
    "\n",
    "$$H(x) = sign(\\sum_{i=1}^{T}h_i(\\mathbf{x}))$$\n",
    "\n",
    "sign 是一个符号函数，也就是说，有超过半数的基分类器正确，则集成分类器就正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们的**“假设”**，由 Hoeffding 不等式可知，集成的错误分类率为\n",
    "\n",
    "$$\\begin{split}\n",
    "P(H(\\mathbf{x})\\neq f(\\mathbf{x})) &=P(I_{h_i \\neq f}> \\frac{T}{2})\\\\\n",
    "&= P(I_{h_i = f}\\leq \\frac{T}{2})\\\\\n",
    "&= \\sum_{k=1}^{[T/2]}{T \\choose k}(1-\\epsilon)^k \\epsilon^{(T-k)}\\\\\n",
    "&\\leq \\exp\\left(-\\frac{T}{2}(1-2\\epsilon)^2\\right)\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见，随着集成中个体分类数目T的增大，集成的错误率将指数级下降，最终趋向于0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定集成包含 T 个个体学习器$\\{h_1, h_2, ... , h_T\\}$,其中 $h_i$ 在 示例 $\\mathbf{x}$ 上的输出为 $h_i(\\mathbf{x})$.本节介绍几种对 $h_i$ 进行结合的常见策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均法\n",
    "\n",
    "对数值型输出，最常见的结合策略是平均法：\n",
    "\n",
    "1. 简单平均法，系数都是 $\\frac{1}{T}$;\n",
    "2. 加权平均法，系数是非负的$w_i$，且 $\\sum_{i=1}^{T} w_i = 1$。\n",
    "\n",
    "实验和应用均显示，**<span class=\"mark\">加权平均法未必一定优于简单平均法</span>。一般而言，在个体学习器性能相差较大室使用加权平均法，而在个体学习器性能接近时采用简单平均法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 投票法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对分类任务来说，从类别标记集合中的N个类别预测出一个标记。最常见的结合策略是投票法：\n",
    "\n",
    "1. **绝对多数投票法**：即预测为类别K的个体学习器个数要超过一个绝对的阈值；\n",
    "2. **相对多数投票法**： 选取相对预测为某类别的个体学习器最多的类别；\n",
    "3. **加权投票法**：在2的基础上，还要考虑每个学习器的权重。\n",
    "\n",
    "标准的绝对多数投票法提供了**“拒绝预测”**的选项，即不存在超过该阈值的类别则拒绝预测，这在可靠性要求比较高的学习任务中是很好的机制。\n",
    "\n",
    "$h_i(\\mathbf{x})$如果输出是**类标记**： $\\{0,1\\}$,称之为“硬投票”；如果输出是**类概率**： $\\in [0,1]$，称之为“软投票”。<span class=\"girk\">在包含了不同类型的异质集成中，不</span><span class=\"girk\">同学习器的类概率不能直接比较，通常化为类标记之后在进行硬投票。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T06:07:31.322000Z",
     "start_time": "2019-01-07T06:07:31.306000Z"
    }
   },
   "source": [
    "### [sklearn.ensemble.VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面通过一个卫星训练集的数据来看下投票器的使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T08:14:48.036000Z",
     "start_time": "2019-01-08T08:14:47.740000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LogisticRegression', 0.86399999999999999)\n",
      "('RandomForestClassifier', 0.872)\n",
      "('SVC', 0.88800000000000001)\n",
      "('VotingClassifier', 0.89600000000000002)\n",
      "('soft_VotingClassifier', 0.91200000000000003)\n",
      "('VotingClassifier', 0.89600000000000002)\n",
      "('GaussianNB', 0.85599999999999998)\n",
      "('MultinomialNB', 0.82399999999999995)\n",
      "('nb_VotingClassifier', 0.82399999999999995)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "gauss_clf = GaussianNB()\n",
    "multi_clf = MultinomialNB()\n",
    "\n",
    "hard_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "# 加了朴素贝叶斯的集成\n",
    "nb_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf), ('gauss_nb', gauss_clf), ('multi_nb', multi_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "# 换成软投票法\n",
    "pro_svc_clf = SVC(gamma=\"auto\", random_state=44, probability=True)\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc_pro', pro_svc_clf)],\n",
    "    voting='soft')\n",
    "\n",
    "# 看看每个分类器在测试集上的准确率：\n",
    "from sklearn.metrics import accuracy_score\n",
    "# gauss_clf, multi_clf, voting_clf_1\n",
    "for clf in (log_clf, rnd_clf, svm_clf, hard_voting_clf, soft_voting_clf): \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    name = clf.__class__.__name__\n",
    "    if clf == soft_voting_clf:\n",
    "        name = 'soft_' + clf.__class__.__name__\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(name, score)\n",
    "    \n",
    "    \n",
    "for clf in (hard_voting_clf, gauss_clf, multi_clf, nb_voting_clf):\n",
    "    if clf == multi_clf or clf == nb_voting_clf:\n",
    "        from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "        ord = OrdinalEncoder()\n",
    "        X_train = ord.fit_transform(X_train)\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    name = clf.__class__.__name__\n",
    "    if clf == nb_voting_clf:\n",
    "        name = 'nb_' + clf.__class__.__name__\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果可知：\n",
    "1. 软投票法相比硬投票法的效果好；\n",
    "2. 加了朴素贝叶斯的集成比不加的效果要差。个人感觉原因在于集成学习的准确率在理论上并没有保证一定比最好的那个基学习器更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多样性增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于集成学习来说，好的集成应该是准确性和多样性的平衡。增强多样性的常见做法是**对数据样本、输入属性、输出表示、算法参数四个维度进行扰动。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据样本扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据样本扰动通常基于**采样法**，例如 Bagging 中使用自助采样，在 Adaboost 中使用序列采样，此类做法简单高效、使用最广。\n",
    "\n",
    "但要注意的是，**数据样本扰动对“不稳定基学习器”很有效，这里的“不稳定”含义是，学习器中很多关键步骤都涉及到数据样本整体的统计特征，所以样本的扰动会影响学习器的生成，比如决策树、神经网络等。对一些诸如线性学习器、支持向量机、朴素贝叶斯、K近邻学习器等“稳定学习器”，数据样本扰动是不敏感的。需要使用其他的多样性增强方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以在涉及到数据样本扰动的 Bagging 中，决策树被经常用来做基学习器，就是因为它属于不稳定学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T05:50:30.031000Z",
     "start_time": "2019-01-05T05:50:30.021000Z"
    }
   },
   "source": [
    "### 输入属性扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从初始属性集中抽取若干个属性子集，再基于每一个属性子集训练一个基学习器，这是输入属性的扰动。**对包含大量属性的数据来说，进行输入属性扰动不仅能产生多样性大的个体，还会因属性的减少大幅节省时间开销；若数据属性比较少，不适合这种方法。**\n",
    "\n",
    "随机子空间算法和随机森林就依赖于输入属性扰动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出表示扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此类做法的基本思路是对输出表示进行操纵来增强多样性，如：\n",
    "1. “翻转法”：随机改变一些训练样本的标记；\n",
    "2. “输出调制法”：将分类输出转化为回归输出后构建个体学习器；\n",
    "3. ECOC法： 利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法参数扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基学习器一般都有参数需要设置，例如神经网络的隐层神经元数、初始连接权值等，通过随机设置不同的参数，往往可以产生差别较大的个体学习器。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47px",
    "left": "1207px",
    "top": "110px",
    "width": "190px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
