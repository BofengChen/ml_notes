{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性判别分析(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性判别分析(Linear Discriminant Analysis, 简称 LDA)是一个常用的**有监督线性降维方法**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 的思想非常朴素：\n",
    "\n",
    "    1. 类别监督降维：给定训练样例集，设法将样例投影到一条直线上，使得同类样例投影点尽可能接近、异类样例投影点尽可能远离；\n",
    "    2.在对新样本进行分类时，将其投影到根据训练集所得到的降维后的直线上，再根据投影点的位置来确定新样本的类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/LDA.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 数学模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以二分类问题为例进行讨论。\n",
    "\n",
    "给定数据集 $D = \\{(\\mathbf{x}_i ,y_i)\\}_{i=1}^m \\; y_i\\in \\{0,1\\}$， 令 $\\mathbf{X}_i$、$\\mathbf{\\mu}_i$、$\\mathbf{\\sum}_i$ 分别表示第 $i\\in \\{0,1\\}$ 类示例的集合、均值向量、协方差矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 两类数据的均值向量在直线 $y = \\mathbf{w}^T\\mathbf{x}$ 上的投影分别是 $\\mathbf{w}^T\\mathbf{\\mu}_0$ 和 $\\mathbf{w}^T\\mathbf{\\mu}_1$；\n",
    "* 若将所有的样本点都投影到直线上，则两类样本的协方差分别为 $\\mathbf{w}^T\\mathbf{\\sum}_0\\mathbf{w}$ 和 $\\mathbf{w}^T\\mathbf{\\sum}_1\\mathbf{w}$.\n",
    "\n",
    "由于直线是一维空间，因此  $\\mathbf{w}^T\\mathbf{\\mu}_0$、$\\mathbf{w}^T\\mathbf{\\mu}_1$、 $\\mathbf{w}^T\\mathbf{\\sum}_0\\mathbf{w}$、$\\mathbf{w}^T\\mathbf{\\sum}_1\\mathbf{w}$ 均为实数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 为了使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即\n",
    "\n",
    "$$ \\mathbf{w}^T\\mathbf{\\sum}_0\\mathbf{w}+\\mathbf{w}^T\\mathbf{\\sum}_1\\mathbf{w}$$尽可能小；\n",
    "\n",
    "2. 欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即 \n",
    "\n",
    "$$||\\mathbf{w}^T\\mathbf{\\mu}_0-\\mathbf{w}^T\\mathbf{\\mu}_1||^2_2$$\n",
    "\n",
    "尽可能大（**其实本质上是定义两个集合的距离，可以类似聚类章节，采用其他的方式来定义**）。\n",
    "\n",
    "同时考虑以上两点，则可以得到最大化目标\n",
    "\n",
    "$$\\mathbf{J} = \\frac{||\\mathbf{w}^T\\mathbf{\\mu}_0-\\mathbf{w}^T\\mathbf{\\mu}_1||^2_2}{\\mathbf{w}^T\\mathbf{\\sum}_0\\mathbf{w}+\\mathbf{w}^T\\mathbf{\\sum}_1\\mathbf{w}}$$\n",
    "\n",
    "假设 $\\mathbf{a}^T\\mathbf{b}$ 是实数，则有\n",
    "\n",
    "$$||\\mathbf{a}^T\\mathbf{b}||^2 = (\\mathbf{a}^T\\mathbf{b})^T\\cdot (\\mathbf{a}^T\\mathbf{b}) = (\\mathbf{a}^T\\mathbf{b})\\cdot (\\mathbf{a}^T\\mathbf{b})^T$$\n",
    "\n",
    "因此\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{J} &= \\frac{||\\mathbf{w}^T(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)||^2_2}{\\mathbf{w}^T(\\mathbf{\\sum}_0+\\mathbf{\\sum}_1)\\mathbf{w}}\\\\\n",
    "&= \\frac{\\mathbf{w}^T(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)\\left(\\mathbf{w}^T(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)\\right)^T}{\\mathbf{w}^T(\\mathbf{\\sum}_0+\\mathbf{\\sum}_1)\\mathbf{w}}\\\\\n",
    "&=\\frac{\\mathbf{w}^T(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)^T\\mathbf{w}}{\\mathbf{w}^T(\\mathbf{\\sum}_0+\\mathbf{\\sum}_1)\\mathbf{w}}\\\\\n",
    "\\end{split}\\tag{LDA-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义**“类内散度矩阵”:**\n",
    "\n",
    "$$ \\mathbf{S}_w = \\mathbf{\\sum}_0+\\mathbf{\\sum}_1 = \\sum_{\\mathbf{x}\\in \\mathbf{X}_0}(\\mathbf{x}-\\mathbf{\\mu}_0)(\\mathbf{x}-\\mathbf{\\mu}_0)^T + \\sum_{\\mathbf{x}\\in \\mathbf{X}_1}(\\mathbf{x}-\\mathbf{\\mu}_1)(\\mathbf{x}-\\mathbf{\\mu}_1)^T $$\n",
    "\n",
    "* 定义**“类间散度矩阵”:**\n",
    "\n",
    "$$\\mathbf{S}_b = (\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则 (LDA-1) 可以改写为：\n",
    "\n",
    "$$\\mathbf{J} = \\frac{\\mathbf{w}^T\\mathbf{S}_b\\mathbf{w}}{\\mathbf{w}^T\\mathbf{S}_w\\mathbf{w}}\\tag{LDA-2}$$\n",
    "\n",
    "这就是 LDA 欲最大化的目标，即 $\\mathbf{S}_b$ 与 $\\mathbf{S}_w$ 的“[广义瑞利商](https://www.cnblogs.com/pinard/p/6244265.html)”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrange 极值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何确定 $\\mathbf{w}$ 呢？注意到 (LDA-2) 的分子和分母都是关于 $\\mathbf{w}$ 的二次项，所以式 (LDA-2) 的解与 $\\mathbf{w}$ 长度无关，只和方向相关。不失一般性，令 $\\mathbf{w}^T\\mathbf{S}_w\\mathbf{w}=1$，则式 (LDA-2) 等价于\n",
    "\n",
    "$$\\begin{split}\n",
    "&\\min_{\\mathbf{w}} \\; -\\mathbf{w}^T\\mathbf{S}_b\\mathbf{w}\\\\\n",
    "&s.t. \\;\\mathbf{w}^T\\mathbf{S}_w\\mathbf{w}=1\n",
    "\\end{split} \\tag{LDA-3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对(LDA-3) 使用拉格朗日乘子法可得\n",
    "\n",
    "$$f(\\mathbf{w}, \\lambda) = -\\mathbf{w}^T\\mathbf{S}_b\\mathbf{w} + \\lambda(\\mathbf{w}^T\\mathbf{S}_w\\mathbf{w}-1)$$\n",
    "\n",
    "由 $\\frac{\\partial f}{\\partial \\mathbf{w}} = 0$ 可得（我没有严格推导）\n",
    "\n",
    "$$\\mathbf{S}_b\\mathbf{w} = \\lambda\\mathbf{S}_w\\mathbf{w} \\tag{LDA-4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到 \n",
    "$$\\begin{split}\n",
    "\\mathbf{S}_b\\mathbf{w} &= (\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)^T\\mathbf{w}\\\\\n",
    "&= (\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)\\left(\\mathbf{w}^T(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)\\right)^T\\\\\n",
    "&=C(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)\n",
    "\\end{split}$$ \n",
    "\n",
    "其中 C 是常数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以 $\\mathbf{S}_b\\mathbf{w}$ 的方向恒为 $\\mathbf{\\mu}_0-\\mathbf{\\mu}_1$,不妨令 $C = \\lambda$，则有\n",
    "\n",
    "$$\\mathbf{S}_b\\mathbf{w} =\\lambda(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)$$\n",
    "\n",
    "带入 (LDA-4) 可得\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{S}_w^{-1}(\\mathbf{\\mu}_0-\\mathbf{\\mu}_1)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑到数值解的稳定性（？）,在实践中通常是对 $\\mathbf{S}_w$ 进行奇异值分解，即 $\\mathbf{S}_w = \\mathbf{U}\\sum^{-1}\\mathbf{V}^T$，这里 $\\sum$ 是一个对角矩阵，对角线上的元素是 $\\mathbf{S}_w$ 的奇异值，然后再由  $\\mathbf{S}_w^{-1} = \\mathbf{V}\\sum^{-1}\\mathbf{U}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，**LDA 可以从贝叶斯决策理论的角度来阐释，并可证明，当两个类别的数据同先验、满足高斯分布且协方差相等时， LDA 可以达到最优分类。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [sklearn.lda.LDA](https://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html#sklearn.lda.LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 多分类情形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将 LDA 推广到多分类任务。假定存在 N 个类，且第 i 类示例数为 $m_i$，我们先定义“全局散度矩阵”\n",
    "\n",
    "$$ \\mathbf{S}_t = \\sum_{i=1}^m (\\mathbf{x}_i-\\mathbf{\\mu})(\\mathbf{x}_i-\\mathbf{\\mu})^T$$\n",
    "\n",
    "其中 $\\mu$ 是所有示例的均值向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将类内散度矩阵 $\\mathbf{S}_w$ 重新定义为每个类别的散度矩阵之和，即\n",
    "\n",
    "$$\\mathbf{S}_w = \\sum_{i=1}^N \\mathbf{S}_{w_i}$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\\mathbf{S}_{w_i} = \\sum_{\\mathbf{x}\\in \\mathbf{X}_i}(\\mathbf{x}-\\mathbf{\\mu}_i)(\\mathbf{x}_i-\\mathbf{\\mu}_i)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 $\\mathbf{S}_b$ 为\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{S}_b &= \\mathbf{S}_t - \\mathbf{S}_w\\\\\n",
    "& = \\sum_{i=1}^N\\sum_{\\mathbf{x}\\in \\mathbf{X}_i} (\\mathbf{x}-\\mathbf{\\mu})(\\mathbf{x}-\\mathbf{\\mu})^T - \\sum_{i=1}^N\\sum_{\\mathbf{x}\\in \\mathbf{X}_i}(\\mathbf{x}-\\mathbf{\\mu}_i)(\\mathbf{x}_i-\\mathbf{\\mu}_i)^T\\\\\n",
    "&=\\sum_{i=1}^N\\sum_{\\mathbf{x}\\in \\mathbf{X}_i}(\\mathbf{x}\\mathbf{x}^T - \\mathbf{x}\\mathbf{\\mu}^T - \\mathbf{\\mu}\\mathbf{x}^T + \\mathbf{\\mu}\\mathbf{\\mu}^T - \\mathbf{x}\\mathbf{x}^T + \\mathbf{x}\\mathbf{\\mu}^T_i -\\mathbf{\\mu}_i\\mathbf{x}^T + \\mathbf{\\mu}_i\\mathbf{\\mu}_i^T)\\\\\n",
    "&=\\sum_{i=1}^N\\sum_{\\mathbf{x}\\in \\mathbf{X}_i}(- \\mathbf{x}\\mathbf{\\mu}^T - \\mathbf{\\mu}\\mathbf{x}^T + \\mathbf{\\mu}\\mathbf{\\mu}^T + \\mathbf{x}\\mathbf{\\mu}^T_i -\\mathbf{\\mu}_i\\mathbf{x}^T + \\mathbf{\\mu}_i\\mathbf{\\mu}_i^T)\\\\\n",
    "&=\\sum_{i=1}^N(- m_i\\mathbf{\\mu}_i\\mathbf{\\mu}^T - m_i\\mathbf{\\mu}\\mathbf{\\mu}_i^T + m_i\\mathbf{\\mu}\\mathbf{\\mu}^T + m_i\\mathbf{\\mu}_i\\mathbf{\\mu}_i^T - m_i\\mathbf{\\mu}_i\\mathbf{\\mu}_i^T + m_i\\mathbf{\\mu}_i\\mathbf{\\mu}_i^T)\\\\\n",
    "&=\\sum_{i=1}^N m_i(-\\mathbf{\\mu}_i\\mathbf{\\mu}^T - \\mathbf{\\mu}\\mathbf{\\mu}_i^T + \\mathbf{\\mu}\\mathbf{\\mu}^T + \\mathbf{\\mu}_i\\mathbf{\\mu}_i^T)\\\\\n",
    "&=\\sum_{i=1}^N m_i(\\mathbf{\\mu}_i-\\mathbf{\\mu})(\\mathbf{\\mu}_i-\\mathbf{\\mu})^T\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，多分类 LDA 可以有多种实现方法： 使用 $\\mathbf{S}_b,\\mathbf{S}_w,\\mathbf{S}_t$ 三者中的任何两个即可。常见的一种实现是采用优化目标\n",
    "\n",
    "$$\\max_{\\mathbf{W}}\\frac{tr(\\mathbf{W}^T \\mathbf{S}_b \\mathbf{W})}{tr(\\mathbf{W}^T \\mathbf{S}_w \\mathbf{W})}\\tag{LDA-5}$$\n",
    "\n",
    "其中 $\\mathbf{W}\\in R^{d\\times (N-1)}, tr(\\cdot)$ 表示矩阵的迹。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(LDA-5) 可通过如下广义特征值问题求解：\n",
    "\n",
    "$$\\mathbf{S}_b \\mathbf{W} = \\lambda \\mathbf{S}_w \\mathbf{W}$$\n",
    "\n",
    "$\\mathbf{W}$ 的闭式解则是 $\\mathbf{S}_w^{-1}\\mathbf{S}_b$ 的 N-1 个最大广义特征值所对应的特征向量组成的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若将 $\\mathbf{W}$ 视为一个投影矩阵，则多分类 LDA 将样本投影到 N-1 维空间， N-1 通常远小于数据原有的属性数（为了提高样本密度）。于是，可以通过投影来减少样本点的维数，且投影过程中使用了类别信息，因此 LDA 也常被视为一种经典的监督降维技术。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
